{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport glob\nimport codecs\nimport nltk\nfrom tqdm import tqdm\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def represent_text(text,n):\n    # Extracts all character 'n'-grams from  a 'text'\n    if n>0:\n        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n    frequency = defaultdict(int)\n    for token in tokens:\n        frequency[token] += 1\n    return frequency","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_files(path,label):\n    # Reads all text files located in the 'path' and assigns them to 'label' class\n    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n    texts=[]\n    for i,v in enumerate(files):\n        f=codecs.open(v,'r',encoding='utf-8')\n        texts.append((f.read(),label))\n        f.close()\n    return texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_vocabulary(texts,n,ft):\n    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n    occurrences=defaultdict(int)\n    for (text,label) in texts:\n        text_occurrences=represent_text(text,n)\n        for ngram in text_occurrences:\n            if ngram in occurrences:\n                occurrences[ngram]+=text_occurrences[ngram]\n            else:\n                occurrences[ngram]=text_occurrences[ngram]\n    vocabulary=[]\n    for i in occurrences.keys():\n        if occurrences[i]>=ft:\n            vocabulary.append(i)\n    return vocabulary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/pan18crossdomainauthorshipattribution/pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02/pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"infocollection = path+os.sep+'collection-info.json'\nproblems = []\nall_train_texts = []\nall_labels = []\nall_test_texts = []\nwith open(infocollection, 'r') as f:\n    for attrib in json.load(f):\n        problems.append(attrib['problem-name'])\nfor index,problem in enumerate(problems):\n#     print(problem)\n    # Reading information about the problem\n    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n    candidates = []\n    with open(infoproblem, 'r') as f:\n        fj = json.load(f)\n        unk_folder = fj['unknown-folder']\n        for attrib in fj['candidate-authors']:\n            candidates.append(attrib['author-name'])\n    # Building training set\n    train_docs=[]\n    for candidate in candidates:\n        train_docs.extend(read_files(path+os.sep+problem,candidate))\n    train_texts = [text for i,(text,label) in enumerate(train_docs)]\n    train_labels = [label for i,(text,label) in enumerate(train_docs)]\n    test_docs=read_files(path+os.sep+problem,unk_folder)\n    test_texts = [text for i,(text,label) in enumerate(test_docs)]\n    \n#     print('\\t', len(candidates), 'candidate authors')\n#     print('\\t', len(train_texts), 'known texts')\n    all_train_texts.append(train_texts)\n    all_labels.append(train_labels)\n    all_test_texts.append(test_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = all_train_texts[0]\ntest1 = all_test_texts[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_en = preprocessing.LabelEncoder()\nlabels1 = lab_en.fit_transform(all_labels[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meta Features:"},{"metadata":{"trusted":false},"cell_type":"code","source":"sent_len = []\nword_len = []\nword_num = []\nsingle_num = []\npunct_num = []\ntit_num = []\nstop_num = []\nfor paragraph in train1:\n    ## average lenth of sentences\n    sent_len.append(np.mean(list(map(\n        lambda x: len(x.split()), sent_tokenize(paragraph)))))\n    ## average lenth of words\n    word_len.append(np.mean(list(map(\n        lambda x: len(str(x)), word_tokenize(paragraph)))))\n    ##number of words\n    word_num.append(len(word_tokenize(paragraph)))\n    ##number of single words\n    single_num.append(len([w for w in set(word_tokenize(paragraph)) if w not in punctuation]))\n    ##number of sentences\n\n    ## average number of punctuation in a sentence\n    punct_num.append(np.mean(list(map(\n        lambda x: len([p for p in str(x) if p in punctuation]),sent_tokenize(paragraph)))))\n    ##average number of  titles words\n    tit_num.append(np.mean(list(map(\n        lambda x: len([t for t in str(x) if t.istitle()]),sent_tokenize(paragraph)))))\n    ##number of stopwords\n    stop_num.append(np.mean(list(map(\n        lambda x: len([t for t in str(x) if t in eng_stopwords]),sent_tokenize(paragraph)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_sent_len = []\ntest_word_len = []\ntest_word_num = []\ntest_single_num = []\ntest_punct_num = []\ntest_tit_num = []\ntest_stop_num = []\nfor paragraph in test1:\n    ## average lenth of sentences\n    test_sent_len.append(np.mean(list(map(\n        lambda x: len(x.split()), sent_tokenize(paragraph)))))\n    ## average lenth of words\n    test_word_len.append(np.mean(list(map(\n        lambda x: len(str(x)), word_tokenize(paragraph)))))\n    ##number of words\n    test_word_num.append(len(word_tokenize(paragraph)))\n    ##number of single words\n    test_single_num.append(len([w for w in set(word_tokenize(paragraph)) if w not in punctuation]))\n    ##number of sentences\n\n    ## average number of punctuation in a sentence\n    test_punct_num.append(np.mean(list(map(\n        lambda x: len([p for p in str(x) if p in punctuation]),sent_tokenize(paragraph)))))\n    ##average number of  titles words\n    test_tit_num.append(np.mean(list(map(\n        lambda x: len([t for t in str(x) if t.istitle()]),sent_tokenize(paragraph)))))\n    ##number of stopwords\n    test_stop_num.append(np.mean(list(map(\n        lambda x: len([t for t in str(x) if t in eng_stopwords]),sent_tokenize(paragraph)))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Based Features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train1, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\nall_tfidf = tfv.fit_transform(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"log = LogisticRegression()\nlog_parameters = { 'C':np.arange(1, 5, 2), 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\nlog_clf = GridSearchCV(log, log_parameters, cv=5, n_jobs=-1)\nlog_clf.fit(xtrain_tfv, ytrain)\nprint(log_clf.best_params_)\nlog_model = log_clf.best_estimator_\nmulticlass_logloss(yvalid, log_model.predict_proba(xvalid_tfv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"log = LogisticRegression()\nlog_parameters = { 'C':np.arange(1, 5, 2), 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\nlog_clf = GridSearchCV(log, log_parameters, cv=5, n_jobs=-1)\nlog_clf.fit(xtrain_ctv, ytrain)\nprint(log_clf.best_params_)\nlog_model = log_clf.best_estimator_\nmulticlass_logloss(yvalid, log_model.predict_proba(xvalid_ctv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svc = SVC()\nsvc.probability = True\nsvc_parameters = {'kernel':('linear', 'rbf'), 'C':np.arange(1, 10, 2), 'gamma':np.arange(0.125, 4, 0.5)}\nsvc_clf = GridSearchCV(svc, svc_parameters, cv=5, n_jobs=-1)\nsvc_clf.fit(xtrain_tfv, ytrain)\nprint(svc_clf.best_params_)\nsvc_model = svc_clf.best_estimator_\nmulticlass_logloss(yvalid, svc_model.predict_proba(xvalid_tfv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"svc = SVC()\nsvc.probability = True\nsvc_parameters = {'kernel':('linear', 'rbf'), 'C':np.arange(1, 10, 2), 'gamma':np.arange(0.125, 4, 0.5)}\nsvc_clf = GridSearchCV(svc, svc_parameters, cv=5, n_jobs=-1)\nsvc_clf.fit(xtrain_ctv, ytrain)\nprint(svc_clf.best_params_)\nsvc_model = svc_clf.best_estimator_\nmulticlass_logloss(yvalid, svc_model.predict_proba(xvalid_ctv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf_parameters = {'n_estimators':np.arange(35,50,3), 'max_depth':np.arange(4,9,2), 'min_samples_split':np.arange(30,50,5),\n                'min_samples_leaf':np.arange(1,15,3),'max_features':np.arange(0.2,1,0.2)}\nrf_clf = GridSearchCV(rf, rf_parameters, cv=5, n_jobs=-1)\nrf_clf.fit(xtrain_tfv, ytrain)\nprint(rf_clf.best_params_)\nrf_model = rf_clf.best_estimator_\nmulticlass_logloss(yvalid, rf_model.predict_proba(xvalid_tfv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf_parameters = {'n_estimators':np.arange(35,50,3), 'max_depth':np.arange(4,9,2), 'min_samples_split':np.arange(30,50,5),\n                'min_samples_leaf':np.arange(1,15,3),'max_features':np.arange(0.2,1,0.2)}\nrf_clf = GridSearchCV(rf, rf_parameters, cv=5, n_jobs=-1)\nrf_clf.fit(xtrain_ctv, ytrain)\nprint(rf_clf.best_params_)\nrf_model = rf_clf.best_estimator_\nmulticlass_logloss(yvalid, rf_model.predict_proba(xvalid_ctv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nxgb_clf = xgb.XGBClassifier(nthread=10, learning_rate=0.1)\nxgb_parameters = {'max_depth':np.arange(1,9,2), 'n_estimators':np.arange(1,301,100), \n                  'colsample_bytree' : np.arange(0.3,1,0.3),}\nxgb_Gclf = GridSearchCV(xgb_clf, xgb_parameters, cv=5, n_jobs=-1)\nxgb_Gclf.fit(xtrain_tfv, ytrain)\nprint(xgb_Gclf.best_params_)\nxgb_model = xgb_Gclf.best_estimator_\nmulticlass_logloss(yvalid, xgb_model.predict_proba(xvalid_tfv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_clf = xgb.XGBClassifier(nthread=10, learning_rate=0.1)\nxgb_parameters = {'max_depth':np.arange(1,9,2), 'n_estimators':np.arange(1,301,100), \n                  'colsample_bytree' : np.arange(0.3,1,0.3),}\nxgb_Gclf = GridSearchCV(xgb_clf, xgb_parameters,n_jobs=-1)\nxgb_Gclf.fit(xtrain_ctv, ytrain)\nprint(xgb_Gclf.best_params_)\nxgb_model = xgb_Gclf.best_estimator_\nmulticlass_logloss(yvalid, xgb_model.predict_proba(xvalid_ctv))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word embedding"},{"metadata":{"trusted":false},"cell_type":"code","source":"embeddings_index = {}\nf = open('glove.840B.300d.txt',encoding=\"utf8\")\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"words = str(xtrain[0]).lower()\nwords = word_tokenize(words)\nwords = [w for w in words if not w in eng_stopwords]\nwords = [w for w in words if w.isalpha]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def sent2vec(s):\n    words = str(xtrain[0]).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in eng_stopwords]\n    words = [w for w in words if w.isalpha]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xtrain_glove = np.array([sent2vec(x) for x in xtrain])\nxvalid_glove = np.array([sent2vec(x) for x in xvalid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"log = LogisticRegression()\nlog_parameters = { 'C':np.arange(1, 5, 2), 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\nlog_clf = GridSearchCV(log, log_parameters, cv=5, n_jobs=-1)\nlog_clf.fit(xtrain_glove, ytrain)\nprint(log_clf.best_params_)\nlog_model = log_clf.best_estimator_\nmulticlass_logloss(yvalid, log_model.predict_proba(xvalid_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb_clf = xgb.XGBClassifier(nthread=10,silent=False)\nxgb_parameters = {'max_depth':np.arange(1,8,2), 'n_estimators':np.arange(1,301,50), \n                  'colsample_bytree' : np.arange(0.1,1,0.2)}\nxgb_Gclf = GridSearchCV(xgb_clf, xgb_parameters,n_jobs=-1)\nxgb_Gclf.fit(xtrain_glove, ytrain)\nprint(xgb_Gclf.best_params_)\nxgb_model = xgb_Gclf.best_estimator_\nmulticlass_logloss(yvalid, xgb_model.predict_proba(xvalid_glove))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deep learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.python.keras import utils\nfrom tensorflow.keras.preprocessing import sequence, text\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deep learning"},{"metadata":{"trusted":false},"cell_type":"code","source":"scl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain_enc = utils.to_categorical(ytrain)  #transform to one-hot vector\nyvalid_enc = utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 895\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xtrain_pad[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"too much dropout, it starts to overfit: val_loss is much bigger than train loss. Try Bi-directional LSTM"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GRU"},{"metadata":{"trusted":false},"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predicate","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}