{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport glob\nimport codecs\nimport nltk\nfrom tqdm import tqdm\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def represent_text(text,n):\n    # Extracts all character 'n'-grams from  a 'text'\n    if n>0:\n        tokens = [text[i:i+n] for i in range(len(text)-n+1)]\n    frequency = defaultdict(int)\n    for token in tokens:\n        frequency[token] += 1\n    return frequency","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_files(path,label):\n    # Reads all text files located in the 'path' and assigns them to 'label' class\n    files = glob.glob(path+os.sep+label+os.sep+'*.txt')\n    texts=[]\n    for i,v in enumerate(files):\n        f=codecs.open(v,'r',encoding='utf-8')\n        texts.append((f.read(),label))\n        f.close()\n    return texts","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_vocabulary(texts,n,ft):\n    # Extracts all characer 'n'-grams occurring at least 'ft' times in a set of 'texts'\n    occurrences=defaultdict(int)\n    for (text,label) in texts:\n        text_occurrences=represent_text(text,n)\n        for ngram in text_occurrences:\n            if ngram in occurrences:\n                occurrences[ngram]+=text_occurrences[ngram]\n            else:\n                occurrences[ngram]=text_occurrences[ngram]\n    vocabulary=[]\n    for i in occurrences.keys():\n        if occurrences[i]>=ft:\n            vocabulary.append(i)\n    return vocabulary","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/pan18crossdomainauthorshipattribution/pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02/pan18-cross-domain-authorship-attribution-training-dataset-2017-12-02'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"infocollection = path+os.sep+'collection-info.json'\nproblems = []\nall_train_texts = []\nall_labels = []\nall_test_texts = []\nwith open(infocollection, 'r') as f:\n    for attrib in json.load(f):\n        problems.append(attrib['problem-name'])\nfor index,problem in enumerate(problems):\n#     print(problem)\n    # Reading information about the problem\n    infoproblem = path+os.sep+problem+os.sep+'problem-info.json'\n    candidates = []\n    with open(infoproblem, 'r') as f:\n        fj = json.load(f)\n        unk_folder = fj['unknown-folder']\n        for attrib in fj['candidate-authors']:\n            candidates.append(attrib['author-name'])\n    # Building training set\n    train_docs=[]\n    for candidate in candidates:\n        train_docs.extend(read_files(path+os.sep+problem,candidate))\n    train_texts = [text for i,(text,label) in enumerate(train_docs)]\n    train_labels = [label for i,(text,label) in enumerate(train_docs)]\n    test_docs=read_files(path+os.sep+problem,unk_folder)\n    test_texts = [text for i,(text,label) in enumerate(test_docs)]\n    \n#     print('\\t', len(candidates), 'candidate authors')\n#     print('\\t', len(train_texts), 'known texts')\n    all_train_texts.append(train_texts)\n    all_labels.append(train_labels)\n    all_test_texts.append(test_texts)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = all_train_texts[0]\ntest1 = all_test_texts[0]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lab_en = preprocessing.LabelEncoder()\nlabels1 = lab_en.fit_transform(all_labels[0])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meta Features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_len = []\nsent_len.append(np.mean(list(map(\n            lambda x: len(x.split()), sent_tokenize(train1[0])))))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def meta_features_extractort(corpus):\n    sent_len = []\n    word_len = []\n    word_num = []\n    single_num = []\n    punct_num = []\n    tit_num = []\n    stop_num = []\n    upper_num = []\n    for paragraph in corpus:\n        ## average lenth of sentences\n        sent_len.append(np.mean(list(map(\n            lambda x: len(x.split()), sent_tokenize(paragraph)))))\n        ## average lenth of words\n        word_len.append(np.mean(list(map(\n            lambda x: len(str(x)), word_tokenize(paragraph)))))\n        ##number of words\n        word_num.append(len(word_tokenize(paragraph)))\n        ##number of single words\n        single_num.append(len([w for w in set(word_tokenize(paragraph)) if w not in punctuation]))\n        ## average number of punctuation in a sentence\n        punct_num.append(np.mean(list(map(\n            lambda x: len([p for p in str(x) if p in punctuation]),sent_tokenize(paragraph)))))\n        ##average number of  titles words\n        tit_num.append(np.mean(list(map(\n            lambda x: len([t for t in str(x) if t.istitle()]),sent_tokenize(paragraph)))))\n        ##number of stopwords\n        stop_num.append(np.mean(list(map(\n            lambda x: len([t for t in str(x) if t in eng_stopwords]),sent_tokenize(paragraph)))))\n        ## Number of upper words in the text ##\n        upper_num.append(np.mean(list(map(\n            lambda x: len([t for t in str(x) if t.isupper()]),sent_tokenize(paragraph)))))\n    \n        x = np.array([sent_len,word_len,word_num,single_num,punct_num,tit_num,stop_num,upper_num])\n    \n    return x.T","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_train = meta_features_extractort(train1)\nmeta_testX = meta_features_extractort(test1)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_trainX, meta_validX, ytrain, yvalid = train_test_split(meta_train, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Based Features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train1, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\nall_tfidf = tfv.fit_transform(train1)\ntfv_testX = tfv.transform(test1)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\nall_ctv = ctv.fit_transform(train1)\nctv_testX = ctv.transform(test1)","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word embedding, you need to download https://nlp.stanford.edu/projects/glove/ , or searching GloVe in kaggle(840B 300d) "},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt',encoding=\"utf8\")\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":23,"outputs":[{"output_type":"stream","text":"2196018it [02:50, 12846.41it/s]","name":"stderr"},{"output_type":"stream","text":"Found 2195893 word vectors.\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I decided to transform whole article to a vector\ndef sent2vec(corpus):\n    words = str(corpus).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in eng_stopwords]#delete high frequency words\n    words = [w for w in words if w.isalpha] # only alpha\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    return v / np.sqrt((v ** 2).sum())\n\nwords_vector = np.array([sent2vec(x) for x in train1])\nembedding_testX = np.array([sent2vec(x) for x in test1])","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the features are extracted, now prepare train and valid, data I decided to use train/test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_trainX, meta_validX, meta_trainY, meta_validY = train_test_split(meta_train, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfv_trainX, tfv_validX, tfv_trainY, tfv_validY = train_test_split(all_tfidf, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv_trainX, ctv_validX, ctv_trainY, ctv_validY = train_test_split(all_ctv, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_trainX, embedding_validX, embedding_trainY, embedding_validY = train_test_split(words_vector, labels1, \n                                                  stratify=labels1, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the names of test set: meta_testX,  tfv_testX,  ctv_testX,  embedding_testX"},{"metadata":{},"cell_type":"markdown","source":"now feed them into model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bulid_LR(xtrain,xvalid,ytrain,yvalid,xtest):\n    log = LogisticRegression()\n   \n    log_parameters = { 'C':np.arange(1, 5, 2), 'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n    log_clf = GridSearchCV(log, log_parameters, cv=5, n_jobs=-1)  #寻找最佳参数，这会很慢，如果不想使用，可以自己修改：把log_clf改为log\n    #log.fit(xtrain, ytrain)    \n    log_clf.fit(xtrain, ytrain)\n    print(log_clf.best_params_)\n    log_model = log_clf.best_estimator_\n    loss = multiclass_logloss(yvalid, log_model.predict_proba(xvalid)) # 模型的loss\n    result = log_model.predict(xtest)\n    \n    return result,loss","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bulid_SVM(xtrain,xvalid,ytrain,yvalid,xtest):\n    svc = SVC()\n    svc.probability = True\n    \n    svc_parameters = {'kernel':('linear', 'rbf'), 'C':np.arange(1, 10, 2), 'gamma':np.arange(0.125, 4, 0.5)}\n    svc_clf = GridSearchCV(svc, svc_parameters, cv=5, n_jobs=-1)\n    \n    svc_clf.fit(xtrain, ytrain)\n    print(svc_clf.best_params_)\n    svc_model = svc_clf.best_estimator_\n    loss = multiclass_logloss(yvalid, svc_model.predict_proba(xvalid))\n    result = svc_model.predict(xtest)\n\n    return result,loss","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bulid_RF(xtrain,xvalid,ytrain,yvalid,xtest):\n    rf = RandomForestClassifier()\n    \n    rf_parameters = {'n_estimators':np.arange(35,50,3), 'max_depth':np.arange(4,9,2), 'min_samples_split':np.arange(30,50,5),\n                    'min_samples_leaf':np.arange(1,15,3),'max_features':np.arange(0.2,1,0.2)}\n    rf_clf = GridSearchCV(rf, rf_parameters, cv=5, n_jobs=-1)\n    \n    rf_clf.fit(xtrain, ytrain)\n    print(rf_clf.best_params_)\n    rf_model = rf_clf.best_estimator_\n    loss = multiclass_logloss(yvalid, rf_model.predict_proba(xvalid))\n    result = rf_model.predict(xtest)\n    \n    return result, loss","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bulid_xgb(xtrain,xvalid,ytrain,yvalid,xtest):\n    xgb_clf = xgb.XGBClassifier(nthread=10, learning_rate=0.1)\n    \n    xgb_parameters = {'max_depth':np.arange(1,9,2), 'n_estimators':np.arange(1,301,100), \n                      'colsample_bytree' : np.arange(0.3,1,0.3),}\n    xgb_Gclf = GridSearchCV(xgb_clf, xgb_parameters, cv=5, n_jobs=-1)\n    \n    xgb_Gclf.fit(xtrain, ytrain)\n    print(xgb_Gclf.best_params_)\n    xgb_model = xgb_Gclf.best_estimator_\n    loss = multiclass_logloss(yvalid, xgb_model.predict_proba(xvalid))\n    result = rf_model.predict(xtest)\n    return result, loss","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"try one feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_meta_testY, LR_meta_loss = bulid_LR(meta_trainX, meta_validX, meta_trainY, meta_validY, meta_testX)","execution_count":40,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n","name":"stderr"},{"output_type":"stream","text":"{'C': 3, 'solver': 'newton-cg'}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM_meta_testY, SVM_meta_loss = bulid_SVM(meta_trainX, meta_validX, meta_trainY, meta_validY, meta_testX)","execution_count":41,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-f55dbb7d0cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVM_meta_testY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVM_meta_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbulid_SVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_validX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_trainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_validY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_testX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-3ac7afc4ca2b>\u001b[0m in \u001b[0;36mbulid_SVM\u001b[0;34m(xtrain, xvalid, ytrain, yvalid, xtest)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msvc_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msvc_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msvc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_meta_testY, RF_meta_loss = bulid_RF(meta_trainX, meta_validX, meta_trainY, meta_validY, meta_testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_meta_testY, xgb_meta_loss = bulid_xgb(meta_trainX, meta_validX, meta_trainY, meta_validY, meta_testX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deep learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.python.keras import utils\nfrom tensorflow.keras.preprocessing import sequence, text\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deep learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"scl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain_enc = utils.to_categorical(ytrain)  #transform to one-hot vector\nyvalid_enc = utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 895\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_pad[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"too much dropout, it starts to overfit: val_loss is much bigger than train loss. Try Bi-directional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(9))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predicate","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}